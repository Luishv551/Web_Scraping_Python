{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a952966",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import string\n",
    "from tqdm import tqdm\n",
    "\n",
    "#Utilizando a estrutura de dicionarios, como o site reconhece que não é um ser humano usando, precisamos fazer o request usando a request que achei no site.\n",
    "\n",
    "#Headers encontrados em NETWORK, utilizando o inspecionar, copia-se e converte CURL para PYTHON.\n",
    "headers = {\n",
    "    'authority': 'www.residentevildatabase.com',\n",
    "    'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7',\n",
    "    'accept-language': 'pt-BR,pt;q=0.9',\n",
    "    'cache-control': 'max-age=0',\n",
    "    'sec-ch-ua': '\"Chromium\";v=\"112\", \"Google Chrome\";v=\"112\", \"Not:A-Brand\";v=\"99\"',\n",
    "    'sec-ch-ua-mobile': '?0',\n",
    "    'sec-ch-ua-platform': '\"Linux\"',\n",
    "    'sec-fetch-dest': 'document',\n",
    "    'sec-fetch-mode': 'navigate',\n",
    "    'sec-fetch-site': 'none',\n",
    "    'sec-fetch-user': '?1',\n",
    "    'upgrade-insecure-requests': '1',\n",
    "    'user-agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/112.0.0.0 Safari/537.36',\n",
    "}\n",
    "\n",
    "#Biblioteca BEAUTIFUL SOUP ajuda a fazer queries no HTML buscando coisas baseado em um padrao.\n",
    "#Passando o objeto soup na função criada get_basic_infos.\n",
    "def get_basic_infos(soup):\n",
    "    \n",
    "    #Utilizamos o FIND para encontrar a div com a classe page content, que foi aonde vimos aonde fica a pagina inteira com as informações do personagem.\n",
    "    ems = (soup.find(\"div\", class_=\"td-page-content\")\n",
    "               #Dentro de paragrafo\n",
    "               .find(\"p\")\n",
    "               #Informações que esão dentro do EM\n",
    "               .find_all(\"em\"))\n",
    "                #Ele retorna uma lista com as informações do personagem\n",
    "        \n",
    "    #Criando o dicionario\n",
    "    data = {}\n",
    "    for em in ems:\n",
    "        #Transformamos em dicionario slipitando a string que pegamos com o .text, esse pega o texto dentro do elemento HTML\n",
    "        chave, valor, *_ = em.text.split(\":\")\n",
    "        #A Strip remove os espaços do final e do começo da string\n",
    "        chave = chave.strip(\" \").lower()\n",
    "        if \"nascimento\" in chave:\n",
    "            chave = \"ano de nascimento\"\n",
    "\n",
    "        valor = valor.strip(\" \").lower()\n",
    "        data[chave] = valor\n",
    "    \n",
    "    return data\n",
    "\n",
    "#Buscando as aparições daquele personagem\n",
    "def get_aparitions(soup):\n",
    "    try:\n",
    "        a_tags = (soup.find(\"h4\")\n",
    "                    #Como o nivel de herarquia de h4 e p é o mesmo, utilizamos o find next\n",
    "                    .find_next_sibling(\"p\")\n",
    "                    #Buscando dentro da Ancora do HTML\n",
    "                    .find_all(\"a\"))\n",
    "        \n",
    "        #A funcao retorna uma lista sendo os elementos cada uma das aparições.\n",
    "        return {\"aparicoes\": [i.text for i in a_tags]}\n",
    "    \n",
    "    except AttributeError as err:\n",
    "        return {\"aparicoes\": []}\n",
    "\n",
    "def get_person_infos(url):\n",
    "    \n",
    "    #Procurando o nome do persongaem com o URL, atraves do split separando por barras e transformando em lista, pegamos o ultimo elemento da lista que e o nome no URL e trocamos o - por espaco\n",
    "    name = url.strip(\"/\").split(\"/\")[-1].replace(\"-\", \" \").title()\n",
    "\n",
    "    resp = requests.get(url, headers=headers)\n",
    "    soup = BeautifulSoup(resp.text)\n",
    "\n",
    "    data = {\"nome\": name}\n",
    "    data.update(get_basic_infos(soup))\n",
    "    data.update(get_aparitions(soup))\n",
    "\n",
    "    return pd.Series(data)\n",
    "\n",
    "\n",
    "def get_links_from_letter(letter):\n",
    "    try:\n",
    "        a_tags = (soup.find(\"h3\", string=letter)\n",
    "                    .find_next_sibling(\"p\")\n",
    "                    .find_all(\"a\"))\n",
    "\n",
    "        links = [a['href'] for a in a_tags]\n",
    "        return links\n",
    "    \n",
    "    except AttributeError as err:\n",
    "        return []\n",
    "\n",
    "\n",
    "url = 'https://www.residentevildatabase.com/personagens/'\n",
    "\n",
    "resp = requests.get(url, headers=headers)\n",
    "soup = BeautifulSoup(resp.text)\n",
    "\n",
    "\n",
    "all_links = []\n",
    "for i in string.ascii_uppercase:\n",
    "    all_links.extend(get_links_from_letter(i))\n",
    "\n",
    "\n",
    "all_data = []\n",
    "for i in tqdm(all_links):\n",
    "    all_data.append(get_person_infos(i))\n",
    "\n",
    "\n",
    "df = pd.concat([i.to_frame().T for i in all_data])\n",
    "df.to_csv(\"dados_re.csv\", index=False, sep=\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b357341",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
